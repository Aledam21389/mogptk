<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>mogptk.model API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mogptk.model</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>Source code</summary>
<pre><code class="python">import os
import json
import time
import numpy as np
import gpflow
import tensorflow as tf
from .dataset import DataSet
import matplotlib
import matplotlib.pyplot as plt

from IPython.core.display import HTML

import logging
logging.getLogger(&#39;tensorflow&#39;).propagate = False
tf.logging.set_verbosity(tf.logging.WARN)

class model:
    def __init__(self, name, dataset):
        #&#34;&#34;&#34;
        #Base class for Multi-Output Gaussian process models. See subclasses for instantiation.
        #    
        #Args:
        #    name (str): Name of the model.
        #    dataset (mogptk.dataset.DataSet): DataSet with Data objects for all the channels. When a (list or dict of) Data object is passed, it will automatically be converted to a DataSet.
        #&#34;&#34;&#34;
        
        if not isinstance(dataset, DataSet):
            dataset = DataSet(dataset)
        if dataset.get_output_dims() == 0:
            raise Exception(&#34;dataset must have at least one channel&#34;)
        if len(set(dataset.get_names())) != len(dataset.get_names()):
            raise Exception(&#34;all data channels must have unique names&#34;)
        if len(set(dataset.get_input_dims())) != 1:
            raise Exception(&#34;all data channels must have the same amount of input dimensions&#34;)

        self.name = name
        self.dataset = dataset
        self.graph = tf.Graph()
        self.session = tf.Session(graph=self.graph)
    
    def _build(self, kernel, likelihood, variational, sparse, like_params):
        &#34;&#34;&#34;
        Build the model using the given kernel and likelihood. The variational and sparse booleans decide which GPflow model will be used.

        Args:
            kernel (gpflow.Kernel): Kernel to use.
            likelihood (gpflow.likelihoods): Likelihood to use from GPFlow, if None
                a default exact inference Gaussian likelihood is used.
            variational (bool): If True, use variational inference to approximate
                function values as Gaussian. If False it will use Monte carlo Markov Chain.
            sparse (bool): If True, will use sparse GP regression.
            like_params (dict): Parameters to GPflow likelihood.
        &#34;&#34;&#34;

        x, y = self.dataset.to_kernel()
        with self.graph.as_default():
            with self.session.as_default():
                # Gaussian likelihood
                if likelihood == None:
                    if not sparse:
                        self.model = gpflow.models.GPR(x, y, kernel)
                    else:
                        # TODO: test if induction points are set
                        self.name += &#39; (sparse)&#39;
                        self.model = gpflow.models.SGPR(x, y, kernel)
                # MCMC
                elif not variational:
                    self.likelihood = likelihood(**like_params)
                    if not sparse:
                        self.name += &#39; (MCMC)&#39;
                        self.model = gpflow.models.GPMC(x, y, kernel, self.likelihood)
                    else:
                        self.name += &#39; (sparse MCMC)&#39;
                        self.model = gpflow.models.SGPMC(x, y, kernel, self.likelihood)
                # Variational
                else:
                    self.likelihood = likelihood(**like_params)
                    if not sparse:
                        self.name += &#39; (variational)&#39;
                        self.model = gpflow.models.VGP(x, y, kernel, self.likelihood)
                    else:
                        self.name += &#39; (sparse variational)&#39;
                        self.model = gpflow.models.SVGP(x, y, kernel, self.likelihood)

    ################################################################

    def print_params(self):
        &#34;&#34;&#34;
        Print the parameters of the model in a table.

        Examples:
            &gt;&gt;&gt; model.print_params()
        &#34;&#34;&#34;
        with np.printoptions(precision=3, floatmode=&#39;fixed&#39;):
            table = &#39;&#39;
            for q, params in enumerate(self.get_params()):
                contents = []
                output_dims = self.dataset.get_output_dims()
                for j in range(output_dims):
                    content = &#39;&lt;td&gt;%s&lt;/td&gt;&#39; % (self.dataset[j].get_name(),)

                    values = []
                    for key in params.keys():
                        val = params[key]
                        if val.ndim == 1 and val.shape[0] == output_dims:
                            values.append(&#39;%.3f&#39; % (val[j],))
                        elif val.ndim == 2 and val.shape[1] == output_dims:
                            values.append(str(val[:,j]))
                        else:
                            values.append(str(val))
                    content += &#39;&lt;td&gt;&#39; + &#39;&lt;/td&gt;&lt;td&gt;&#39;.join(values) + &#39;&lt;/td&gt;&#39;
                    contents.append(content)

                header = &#39;&lt;tr&gt;&lt;th&gt;Q=%d&lt;/th&gt;&lt;th&gt;&#39; % (q) + &#39;&lt;/th&gt;&lt;th&gt;&#39;.join(params.keys()) + &#39;&lt;/th&gt;&lt;/tr&gt;&#39;
                contents = &#39;&lt;tr&gt;&#39; + &#39;&lt;/tr&gt;&lt;tr&gt;&#39;.join(contents) + &#39;&lt;/tr&gt;&#39;
                table += header + contents
            display(HTML(&#39;&lt;table&gt;%s&lt;/table&gt;&#39; % (table)))

            likelihood = self.get_likelihood_params()
            table = &#39;&lt;tr&gt;&lt;th&gt;Likelihood&lt;/th&gt;&#39;
            for key in likelihood:
                table += &#39;&lt;th&gt;%s&lt;/th&gt;&#39; % (key,)
            table += &#39;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&#39;
            for key in likelihood:
                table += &#39;&lt;td&gt;%s&lt;/td&gt;&#39; % (likelihood[key],)
            table += &#39;&lt;/tr&gt;&#39;
            display(HTML(&#39;&lt;table&gt;%s&lt;/table&gt;&#39; % (table)))

    def get_params(self):
        &#34;&#34;&#34;
        Returns all parameters set for the kernel per component.

        Examples:
            &gt;&gt;&gt; params = model.get_params()
        &#34;&#34;&#34;
        params = []
        if hasattr(self.model.kern, &#39;kernels&#39;):
            for kernel_i, kernel in enumerate(self.model.kern.kernels):
                params.append({})
                for param_name, param_val in kernel.__dict__.items():
                    if isinstance(param_val, gpflow.params.parameter.Parameter):
                        params[kernel_i][param_name] = param_val.read_value()
        else:
            params.append({})
            for param_name, param_val in self.model.kern.__dict__.items():
                if isinstance(param_val, gpflow.params.parameter.Parameter):
                    params[0][param_name] = param_val.read_value()
        return params

    def get_likelihood_params(self):
        &#34;&#34;&#34;
        Returns all parameters set for the likelihood.

        Examples:
            &gt;&gt;&gt; params = model.get_likelihood_params()
        &#34;&#34;&#34;
        params = {}
        for param_name, param_val in self.model.likelihood.__dict__.items():
            if isinstance(param_val, gpflow.params.parameter.Parameter):
                params[param_name] = param_val.read_value()
        return params

    def get_param(self, q, key):
        &#34;&#34;&#34;
        Gets a kernel parameter for component &#39;q&#39; with key the parameter name.

        Args:
            q (int): Component of kernel.
            key (str): Name of component.
            
        Returns:
            val (numpy.ndarray): Value of parameter.

        Examples:
            &gt;&gt;&gt; val = model.get_param(0, &#39;variance&#39;) # for Q=0 get the parameter called &#39;variance&#39;
        &#34;&#34;&#34;
        if hasattr(self.model.kern, &#39;kernels&#39;):
            if q &lt; 0 or len(self.model.kern.kernels) &lt;= q:
                raise Exception(&#34;qth component %d does not exist&#34; % (q))
            kern = self.model.kern.kernels[q].__dict__
        else:
            if q != 0:
                raise Exception(&#34;qth component %d does not exist&#34; % (q))
            kern = self.model.kern.__dict__
        
        if key not in kern or not isinstance(kern[key], gpflow.params.parameter.Parameter):
            raise Exception(&#34;parameter name &#39;%s&#39; does not exist&#34; % (key))
    
        return kern[key].read_value()

    def set_param(self, q, key, val):
        &#34;&#34;&#34;
        Sets a kernel parameter for component &#39;q&#39; with key the parameter name.

        Args:
            q (int): Component of kernel.
            key (str): Name of component.
            val (float, numpy.ndarray): Value of parameter.

        Examples:
            &gt;&gt;&gt; model.set_param(0, &#39;variance&#39;, np.array([5.0, 3.0])) # for Q=0 set the parameter called &#39;variance&#39;
        &#34;&#34;&#34;
        if isinstance(val, (int, float, list)):
            val = np.array(val)
        if not isinstance(val, np.ndarray):
            raise Exception(&#34;value %s of type %s is not a number type or ndarray&#34; % (val, type(val)))

        if hasattr(self.model.kern, &#39;kernels&#39;):
            if q &lt; 0 or len(self.model.kern.kernels) &lt;= q:
                raise Exception(&#34;qth component %d does not exist&#34; % (q))
            kern = self.model.kern.kernels[q].__dict__
        else:
            if q != 0:
                raise Exception(&#34;qth component %d does not exist&#34; % (q))
            kern = self.model.kern.__dict__

        if key not in kern or not isinstance(kern[key], gpflow.params.parameter.Parameter):
            raise Exception(&#34;parameter name &#39;%s&#39; does not exist&#34; % (key))

        if kern[key].shape != val.shape:
            raise Exception(&#34;parameter name &#39;%s&#39; must have shape %s and not %s&#34; % (key, kern[key].shape, val.shape))

        with self.graph.as_default():
            with self.session.as_default():
                kern[key].assign(val)

    def set_likelihood_param(self, key, val):
        &#34;&#34;&#34;
        Sets a likelihood parameter with key the parameter name.

        Args:
            key (str): Name of component.
            val (float, ndarray): Value of parameter.

        Examples:
            &gt;&gt;&gt; model.set_likelihood_param(&#39;variance&#39;, np.array([5.0, 3.0])) # set the parameter called &#39;variance&#39;
        &#34;&#34;&#34;
        if isinstance(val, (int, float, list)):
            val = np.array(val)
        if not isinstance(val, np.ndarray):
            raise Exception(&#34;value %s of type %s is not a number type or ndarray&#34; % (val, type(val)))

        likelihood = self.model.likelihood.__dict__
        if key not in likelihood or not isinstance(likelihood[key], gpflow.params.parameter.Parameter):
            raise Exception(&#34;parameter name &#39;%s&#39; does not exist&#34; % (key))

        if likelihood[key].shape != val.shape:
            raise Exception(&#34;parameter name &#39;%s&#39; must have shape %s and not %s&#34; % (key, likelihood[key].shape, val.shape))

        with self.graph.as_default():
            with self.session.as_default():
                likelihood[key].assign(val)

    def fix_param(self, key):
        &#34;&#34;&#34;
        Make parameter untrainable (undo with `unfix_param`).

        Args:
            key (str): Name of the parameter.

        Examples:
            &gt;&gt;&gt; model.fix_param(&#39;variance&#39;)
        &#34;&#34;&#34;
        if hasattr(self.model.kern, &#39;kernels&#39;):
            for kernel_i, kernel in enumerate(self.model.kern.kernels):
                for param_name, param_val in kernel.__dict__.items():
                    if param_name == key and isinstance(param_val, gpflow.params.parameter.Parameter):
                        getattr(self.model.kern.kernels[kernel_i], param_name).trainable = False
        else:
            for param_name, param_val in self.model.kern.__dict__.items():
                if param_name == key and isinstance(param_val, gpflow.params.parameter.Parameter):
                    getattr(self.model.kern, param_name).trainable = False

    def unfix_param(self, key):
        &#34;&#34;&#34;
        Make parameter trainable (that was previously fixed, see `fix_param`).

        Args:
            key (str): Name of the parameter.

        Examples:
            &gt;&gt;&gt; model.unfix_param(&#39;variance&#39;)
        &#34;&#34;&#34;
        if hasattr(self.model.kern, &#39;kernels&#39;):
            for kernel_i, kernel in enumerate(self.model.kern.kernels):
                for param_name, param_val in kernel.__dict__.items():
                    if param_name == key and isinstance(param_val, gpflow.params.parameter.Parameter):
                        getattr(self.model.kern.kernels[kernel_i], param_name).trainable = True
        else:
            for param_name, param_val in self.model.kern.__dict__.items():
                if param_name == key and isinstance(param_val, gpflow.params.parameter.Parameter):
                    getattr(self.model.kern, param_name).trainable = True

    def save_params(self, filename):
        &#34;&#34;&#34;
        Save model parameters to a given file that can then be loaded with `load_params()`.

        Args:
            filename (str): Filename to save to, automatically appends &#39;.params&#39;.

        Examples:
            &gt;&gt;&gt; model.save_params(&#39;filename&#39;)
        &#34;&#34;&#34;
        filename += &#34;.&#34; + self.name + &#34;.params&#34;

        try:
            os.remove(filename)
        except OSError:
            pass
        
        class NumpyEncoder(json.JSONEncoder):
            def default(self, obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                return json.JSONEncoder.default(self, obj)

        data = {
            &#39;model&#39;: self.__class__.__name__,
            &#39;likelihood&#39;: self.get_likelihood_params(),
            &#39;params&#39;: self.get_params()
        }
        with open(filename, &#39;w&#39;) as w:
            json.dump(data, w, cls=NumpyEncoder)

    def load_params(self, filename):
        &#34;&#34;&#34;
        Load model parameters from a given file that was previously saved with `save_params()`.

        Args:
            filename (str): Filename to load from, automatically appends &#39;.params&#39;.

        Examples:
            &gt;&gt;&gt; model.load_params(&#39;filename&#39;)
        &#34;&#34;&#34;
        filename += &#34;.&#34; + self.name + &#34;.params&#34;

        with open(filename) as r:
            data = json.load(r)

            if not isinstance(data, dict) or &#39;model&#39; not in data or &#39;likelihood&#39; not in data or &#39;params&#39; not in data:
                raise Exception(&#39;parameter file has bad format&#39;)
            if not isinstance(data[&#39;params&#39;], list) or not all(isinstance(param, dict) for param in data[&#39;params&#39;]):
                raise Exception(&#39;parameter file has bad format&#39;)

            if data[&#39;model&#39;] != self.__class__.__name__:
                raise Exception(&#34;parameter file uses model &#39;%s&#39; which is different from current model &#39;%s&#39;&#34; % (data[&#39;model&#39;], self.__class__.__name__))

            cur_params = self.get_params()
            if len(data[&#39;params&#39;]) != len(cur_params):
                raise Exception(&#34;parameter file uses model with %d kernels which is different from current model that uses %d kernels, is the model&#39;s Q different?&#34; % (len(data[&#39;params&#39;]), len(cur_params)))

            for key, val in data[&#39;likelihood&#39;].items():
                self.set_likelihood_param(key, val)

            for q, param in enumerate(data[&#39;params&#39;]):
                for key, val in param.items():
                    self.set_param(q, key, val)

    def train(
        self,
        method=&#39;L-BFGS-B&#39;,
        tol=1e-6,
        maxiter=2000,
        opt_params={},
        params={},
        export_graph=False):
        &#34;&#34;&#34;
        Trains the model using the kernel and its parameters.

        For different optimizers, see scipy.optimize.minimize.
        It can be bounded by a maximum number of iterations, disp will output final
        optimization information. When using the &#39;Adam&#39; optimizer, a
        learning_rate can be set.

        Args:
            method (str): Optimizer to use, if &#34;Adam&#34; is chosen,
                gpflow.training.Adamoptimizer will be used, otherwise the passed scipy
                optimizer is used. Defaults to scipy &#39;L-BFGS-B&#39;.
            tol (float): Tolerance for optimizer. Defaults to 1e-6.
            maxiter (int): Maximum number of iterations. Defaults to 2000.
            opt_params (dict): Aditional dictionary with parameters on optimizer.
                If method is &#39;Adam&#39; see:
                https://github.com/GPflow/GPflow/blob/develop/gpflow/training/tensorflow_optimizer.py
                If method is in scipy-optimizer see:
                https://github.com/GPflow/GPflow/blob/develop/gpflow/training/scipy_optimizer.py
                https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html
            params (dict): Additional dictionary with parameters to minimize. 
                See https://github.com/GPflow/GPflow/blob/develop/gpflow/training/optimizer.py
                for more details.
            export_graph (bool): Default to False.

        Examples:
            &gt;&gt;&gt; model.train(tol=1e-6, maxiter=1e5)
            
            &gt;&gt;&gt; model.train(method=&#39;Adam&#39;, opt_params={...})
        &#34;&#34;&#34;
        start_time = time.time()
        with self.graph.as_default():
            with self.session.as_default():
                if export_graph:
                    def get_tensor(name):
                        return self.graph.get_tensor_by_name(&#39;GPR-&#39; + self.model._index + &#39;/likelihood_1/&#39; + name + &#39;:0&#39;)
                    writer = tf.summary.FileWriter(&#34;log&#34;, self.graph)
                    K_summary = tf.summary.histogram(&#39;K&#39;, get_tensor(&#39;K&#39;))

                step_i = 0
                def step(theta):
                    nonlocal step_i
                    if export_graph:
                        writer.add_summary(self.session.run(K_summary), step_i)
                    step_i += 1

                if method == &#34;Adam&#34;:
                    opt = gpflow.training.AdamOptimizer(**opt_params)
                    opt.minimize(self.model, anchor=True, **params)
                else:
                    opt = gpflow.train.ScipyOptimizer(method=method, tol=tol, **opt_params)
                    opt.minimize(self.model, anchor=True, step_callback=step, maxiter=maxiter, disp=True, **params)

        print(&#34;Done in %.1f minutes&#34; % ((time.time() - start_time)/60))

    ################################################################################
    # Predictions ##################################################################
    ################################################################################

    def predict(self, x=None, plot=False):
        &#34;&#34;&#34;
        Predict with model.

        Will make a prediction using x as input. If no input value is passed, the prediction will 
        be made with atribute self.X_pred that can be setted with other functions.
        It returns the X, Y_mu, Y_var values per channel.

        Args:
            x_pred (list, dict): Dictionary where keys are channel index and elements numpy arrays with channel inputs.

        Examples:
            &gt;&gt;&gt; model.predict(plot=True)
        &#34;&#34;&#34;
        if x is not None:
            self.dataset.set_pred(self.name, x)

        x = self.dataset.to_kernel_pred()
        if len(x) == 0:
                raise Exception(&#39;no prediction x range set, use pred_x argument or set manually using DataSet.set_pred() or Data.set_pred()&#39;)
        with self.graph.as_default():
            with self.session.as_default():
                mu, var = self.model.predict_f(x)
        self.dataset.from_kernel_pred(self.name, mu, var)
        
        if plot:
            self.plot_prediction()

    # TODO
    def plot_prediction(self, grid=None, figsize=(12, 8), ylims=None, names=None, title=&#39;&#39;, ret_fig=False):

        &#34;&#34;&#34;
        Plot training points, all data and prediction for training range for all channels.

        Args:
            grid (tuple) : Tuple with the 2 dimensions of the grid.
            figsize(tuple): Figure size, default to (12, 8).
            ylims(list): List of tuples with limits for Y axis for
                each channel.
            Names(list): List of the names of each title.
            title(str): Title of the plot.
            ret_fig(bool): If true returns the matplotlib figure, 
                array of axis and dictionary with all the points used.

        TODO: Add case for single output SM kernel.
        &#34;&#34;&#34;
        # get data
        x_train, y_train = self.dataset.get_data()
        x_all, y_all = self.dataset.get_all()
        x_pred, mu, lower, upper = self.dataset.get_pred(self.name)

        n_dim = self.dataset.get_output_dims()
        if n_dim == 1:
            grid = (1, 1)
        elif grid is None:
            grid = (int(np.ceil(n_dim/2)), 2)

        if (grid[0] * grid[1]) &lt; n_dim:
            raise Exception(&#39;grid not big enough for all channels&#39;)

        fig, axes = plt.subplots(grid[0], grid[1], sharex=False, figsize=figsize)
        axes = np.array(axes).reshape(-1)

        colors = list(matplotlib.colors.TABLEAU_COLORS)
        for i in range(n_dim):
            axes[i].plot(x_train[i][:,0], y_train[i], &#39;.k&#39;, label=&#39;Train&#39;, ms=5)
            axes[i].plot(x_all[i][:,0], y_all[i], &#39;--&#39;, label=&#39;Test&#39;, c=&#39;gray&#39;,lw=1.4, zorder=5)
            
            axes[i].plot(x_pred[i][:,0], mu[i], label=&#39;Post.Mean&#39;, c=colors[i], zorder=1)
            axes[i].fill_between(x_pred[i][:,0].reshape(-1),
                lower[i],
                upper[i],
                label=&#39;95% c.i&#39;,
                color=colors[i],
                alpha=0.4)
            
            axes[i].xaxis.set_major_locator(plt.MaxNLocator(6))

            formatter = matplotlib.ticker.FuncFormatter(lambda x,pos: self.dataset.get(i).formatters[0]._format(x))
            axes[i].xaxis.set_major_formatter(formatter)

            # set channels name
            if names is not None:
                axes[i].set_title(names[i])
            else:
                channel_name = self.dataset.get_names()[i]
                if channel_name != &#39;&#39;:
                    axes[i].set_title(channel_name)
                elif n_dim == 1:
                    pass
                else:
                    axes[i].set_title(&#39;Channel &#39; + str(i))

            # set y lims
            if ylims is not None:
                axes[i].set_ylim(ylims[i]) 
            
        plt.suptitle(title, y=1.02)
        plt.tight_layout()

        data_dict = {
            &#39;x_train&#39;:x_train,
            &#39;y_train&#39;:y_train,
            &#39;x_all&#39;:x_all,
            &#39;y_all&#39;:y_all,
            &#39;y_pred&#39;:mu,
            &#39;low_ci&#39;:lower,
            &#39;hi_ci&#39;:upper,
        }

        if ret_fig:
            return fig, axes, data_dict</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mogptk.model.model"><code class="flex name class">
<span>class <span class="ident">model</span></span>
<span>(</span><span>name, dataset)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class model:
    def __init__(self, name, dataset):
        #&#34;&#34;&#34;
        #Base class for Multi-Output Gaussian process models. See subclasses for instantiation.
        #    
        #Args:
        #    name (str): Name of the model.
        #    dataset (mogptk.dataset.DataSet): DataSet with Data objects for all the channels. When a (list or dict of) Data object is passed, it will automatically be converted to a DataSet.
        #&#34;&#34;&#34;
        
        if not isinstance(dataset, DataSet):
            dataset = DataSet(dataset)
        if dataset.get_output_dims() == 0:
            raise Exception(&#34;dataset must have at least one channel&#34;)
        if len(set(dataset.get_names())) != len(dataset.get_names()):
            raise Exception(&#34;all data channels must have unique names&#34;)
        if len(set(dataset.get_input_dims())) != 1:
            raise Exception(&#34;all data channels must have the same amount of input dimensions&#34;)

        self.name = name
        self.dataset = dataset
        self.graph = tf.Graph()
        self.session = tf.Session(graph=self.graph)
    
    def _build(self, kernel, likelihood, variational, sparse, like_params):
        &#34;&#34;&#34;
        Build the model using the given kernel and likelihood. The variational and sparse booleans decide which GPflow model will be used.

        Args:
            kernel (gpflow.Kernel): Kernel to use.
            likelihood (gpflow.likelihoods): Likelihood to use from GPFlow, if None
                a default exact inference Gaussian likelihood is used.
            variational (bool): If True, use variational inference to approximate
                function values as Gaussian. If False it will use Monte carlo Markov Chain.
            sparse (bool): If True, will use sparse GP regression.
            like_params (dict): Parameters to GPflow likelihood.
        &#34;&#34;&#34;

        x, y = self.dataset.to_kernel()
        with self.graph.as_default():
            with self.session.as_default():
                # Gaussian likelihood
                if likelihood == None:
                    if not sparse:
                        self.model = gpflow.models.GPR(x, y, kernel)
                    else:
                        # TODO: test if induction points are set
                        self.name += &#39; (sparse)&#39;
                        self.model = gpflow.models.SGPR(x, y, kernel)
                # MCMC
                elif not variational:
                    self.likelihood = likelihood(**like_params)
                    if not sparse:
                        self.name += &#39; (MCMC)&#39;
                        self.model = gpflow.models.GPMC(x, y, kernel, self.likelihood)
                    else:
                        self.name += &#39; (sparse MCMC)&#39;
                        self.model = gpflow.models.SGPMC(x, y, kernel, self.likelihood)
                # Variational
                else:
                    self.likelihood = likelihood(**like_params)
                    if not sparse:
                        self.name += &#39; (variational)&#39;
                        self.model = gpflow.models.VGP(x, y, kernel, self.likelihood)
                    else:
                        self.name += &#39; (sparse variational)&#39;
                        self.model = gpflow.models.SVGP(x, y, kernel, self.likelihood)

    ################################################################

    def print_params(self):
        &#34;&#34;&#34;
        Print the parameters of the model in a table.

        Examples:
            &gt;&gt;&gt; model.print_params()
        &#34;&#34;&#34;
        with np.printoptions(precision=3, floatmode=&#39;fixed&#39;):
            table = &#39;&#39;
            for q, params in enumerate(self.get_params()):
                contents = []
                output_dims = self.dataset.get_output_dims()
                for j in range(output_dims):
                    content = &#39;&lt;td&gt;%s&lt;/td&gt;&#39; % (self.dataset[j].get_name(),)

                    values = []
                    for key in params.keys():
                        val = params[key]
                        if val.ndim == 1 and val.shape[0] == output_dims:
                            values.append(&#39;%.3f&#39; % (val[j],))
                        elif val.ndim == 2 and val.shape[1] == output_dims:
                            values.append(str(val[:,j]))
                        else:
                            values.append(str(val))
                    content += &#39;&lt;td&gt;&#39; + &#39;&lt;/td&gt;&lt;td&gt;&#39;.join(values) + &#39;&lt;/td&gt;&#39;
                    contents.append(content)

                header = &#39;&lt;tr&gt;&lt;th&gt;Q=%d&lt;/th&gt;&lt;th&gt;&#39; % (q) + &#39;&lt;/th&gt;&lt;th&gt;&#39;.join(params.keys()) + &#39;&lt;/th&gt;&lt;/tr&gt;&#39;
                contents = &#39;&lt;tr&gt;&#39; + &#39;&lt;/tr&gt;&lt;tr&gt;&#39;.join(contents) + &#39;&lt;/tr&gt;&#39;
                table += header + contents
            display(HTML(&#39;&lt;table&gt;%s&lt;/table&gt;&#39; % (table)))

            likelihood = self.get_likelihood_params()
            table = &#39;&lt;tr&gt;&lt;th&gt;Likelihood&lt;/th&gt;&#39;
            for key in likelihood:
                table += &#39;&lt;th&gt;%s&lt;/th&gt;&#39; % (key,)
            table += &#39;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&#39;
            for key in likelihood:
                table += &#39;&lt;td&gt;%s&lt;/td&gt;&#39; % (likelihood[key],)
            table += &#39;&lt;/tr&gt;&#39;
            display(HTML(&#39;&lt;table&gt;%s&lt;/table&gt;&#39; % (table)))

    def get_params(self):
        &#34;&#34;&#34;
        Returns all parameters set for the kernel per component.

        Examples:
            &gt;&gt;&gt; params = model.get_params()
        &#34;&#34;&#34;
        params = []
        if hasattr(self.model.kern, &#39;kernels&#39;):
            for kernel_i, kernel in enumerate(self.model.kern.kernels):
                params.append({})
                for param_name, param_val in kernel.__dict__.items():
                    if isinstance(param_val, gpflow.params.parameter.Parameter):
                        params[kernel_i][param_name] = param_val.read_value()
        else:
            params.append({})
            for param_name, param_val in self.model.kern.__dict__.items():
                if isinstance(param_val, gpflow.params.parameter.Parameter):
                    params[0][param_name] = param_val.read_value()
        return params

    def get_likelihood_params(self):
        &#34;&#34;&#34;
        Returns all parameters set for the likelihood.

        Examples:
            &gt;&gt;&gt; params = model.get_likelihood_params()
        &#34;&#34;&#34;
        params = {}
        for param_name, param_val in self.model.likelihood.__dict__.items():
            if isinstance(param_val, gpflow.params.parameter.Parameter):
                params[param_name] = param_val.read_value()
        return params

    def get_param(self, q, key):
        &#34;&#34;&#34;
        Gets a kernel parameter for component &#39;q&#39; with key the parameter name.

        Args:
            q (int): Component of kernel.
            key (str): Name of component.
            
        Returns:
            val (numpy.ndarray): Value of parameter.

        Examples:
            &gt;&gt;&gt; val = model.get_param(0, &#39;variance&#39;) # for Q=0 get the parameter called &#39;variance&#39;
        &#34;&#34;&#34;
        if hasattr(self.model.kern, &#39;kernels&#39;):
            if q &lt; 0 or len(self.model.kern.kernels) &lt;= q:
                raise Exception(&#34;qth component %d does not exist&#34; % (q))
            kern = self.model.kern.kernels[q].__dict__
        else:
            if q != 0:
                raise Exception(&#34;qth component %d does not exist&#34; % (q))
            kern = self.model.kern.__dict__
        
        if key not in kern or not isinstance(kern[key], gpflow.params.parameter.Parameter):
            raise Exception(&#34;parameter name &#39;%s&#39; does not exist&#34; % (key))
    
        return kern[key].read_value()

    def set_param(self, q, key, val):
        &#34;&#34;&#34;
        Sets a kernel parameter for component &#39;q&#39; with key the parameter name.

        Args:
            q (int): Component of kernel.
            key (str): Name of component.
            val (float, numpy.ndarray): Value of parameter.

        Examples:
            &gt;&gt;&gt; model.set_param(0, &#39;variance&#39;, np.array([5.0, 3.0])) # for Q=0 set the parameter called &#39;variance&#39;
        &#34;&#34;&#34;
        if isinstance(val, (int, float, list)):
            val = np.array(val)
        if not isinstance(val, np.ndarray):
            raise Exception(&#34;value %s of type %s is not a number type or ndarray&#34; % (val, type(val)))

        if hasattr(self.model.kern, &#39;kernels&#39;):
            if q &lt; 0 or len(self.model.kern.kernels) &lt;= q:
                raise Exception(&#34;qth component %d does not exist&#34; % (q))
            kern = self.model.kern.kernels[q].__dict__
        else:
            if q != 0:
                raise Exception(&#34;qth component %d does not exist&#34; % (q))
            kern = self.model.kern.__dict__

        if key not in kern or not isinstance(kern[key], gpflow.params.parameter.Parameter):
            raise Exception(&#34;parameter name &#39;%s&#39; does not exist&#34; % (key))

        if kern[key].shape != val.shape:
            raise Exception(&#34;parameter name &#39;%s&#39; must have shape %s and not %s&#34; % (key, kern[key].shape, val.shape))

        with self.graph.as_default():
            with self.session.as_default():
                kern[key].assign(val)

    def set_likelihood_param(self, key, val):
        &#34;&#34;&#34;
        Sets a likelihood parameter with key the parameter name.

        Args:
            key (str): Name of component.
            val (float, ndarray): Value of parameter.

        Examples:
            &gt;&gt;&gt; model.set_likelihood_param(&#39;variance&#39;, np.array([5.0, 3.0])) # set the parameter called &#39;variance&#39;
        &#34;&#34;&#34;
        if isinstance(val, (int, float, list)):
            val = np.array(val)
        if not isinstance(val, np.ndarray):
            raise Exception(&#34;value %s of type %s is not a number type or ndarray&#34; % (val, type(val)))

        likelihood = self.model.likelihood.__dict__
        if key not in likelihood or not isinstance(likelihood[key], gpflow.params.parameter.Parameter):
            raise Exception(&#34;parameter name &#39;%s&#39; does not exist&#34; % (key))

        if likelihood[key].shape != val.shape:
            raise Exception(&#34;parameter name &#39;%s&#39; must have shape %s and not %s&#34; % (key, likelihood[key].shape, val.shape))

        with self.graph.as_default():
            with self.session.as_default():
                likelihood[key].assign(val)

    def fix_param(self, key):
        &#34;&#34;&#34;
        Make parameter untrainable (undo with `unfix_param`).

        Args:
            key (str): Name of the parameter.

        Examples:
            &gt;&gt;&gt; model.fix_param(&#39;variance&#39;)
        &#34;&#34;&#34;
        if hasattr(self.model.kern, &#39;kernels&#39;):
            for kernel_i, kernel in enumerate(self.model.kern.kernels):
                for param_name, param_val in kernel.__dict__.items():
                    if param_name == key and isinstance(param_val, gpflow.params.parameter.Parameter):
                        getattr(self.model.kern.kernels[kernel_i], param_name).trainable = False
        else:
            for param_name, param_val in self.model.kern.__dict__.items():
                if param_name == key and isinstance(param_val, gpflow.params.parameter.Parameter):
                    getattr(self.model.kern, param_name).trainable = False

    def unfix_param(self, key):
        &#34;&#34;&#34;
        Make parameter trainable (that was previously fixed, see `fix_param`).

        Args:
            key (str): Name of the parameter.

        Examples:
            &gt;&gt;&gt; model.unfix_param(&#39;variance&#39;)
        &#34;&#34;&#34;
        if hasattr(self.model.kern, &#39;kernels&#39;):
            for kernel_i, kernel in enumerate(self.model.kern.kernels):
                for param_name, param_val in kernel.__dict__.items():
                    if param_name == key and isinstance(param_val, gpflow.params.parameter.Parameter):
                        getattr(self.model.kern.kernels[kernel_i], param_name).trainable = True
        else:
            for param_name, param_val in self.model.kern.__dict__.items():
                if param_name == key and isinstance(param_val, gpflow.params.parameter.Parameter):
                    getattr(self.model.kern, param_name).trainable = True

    def save_params(self, filename):
        &#34;&#34;&#34;
        Save model parameters to a given file that can then be loaded with `load_params()`.

        Args:
            filename (str): Filename to save to, automatically appends &#39;.params&#39;.

        Examples:
            &gt;&gt;&gt; model.save_params(&#39;filename&#39;)
        &#34;&#34;&#34;
        filename += &#34;.&#34; + self.name + &#34;.params&#34;

        try:
            os.remove(filename)
        except OSError:
            pass
        
        class NumpyEncoder(json.JSONEncoder):
            def default(self, obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                return json.JSONEncoder.default(self, obj)

        data = {
            &#39;model&#39;: self.__class__.__name__,
            &#39;likelihood&#39;: self.get_likelihood_params(),
            &#39;params&#39;: self.get_params()
        }
        with open(filename, &#39;w&#39;) as w:
            json.dump(data, w, cls=NumpyEncoder)

    def load_params(self, filename):
        &#34;&#34;&#34;
        Load model parameters from a given file that was previously saved with `save_params()`.

        Args:
            filename (str): Filename to load from, automatically appends &#39;.params&#39;.

        Examples:
            &gt;&gt;&gt; model.load_params(&#39;filename&#39;)
        &#34;&#34;&#34;
        filename += &#34;.&#34; + self.name + &#34;.params&#34;

        with open(filename) as r:
            data = json.load(r)

            if not isinstance(data, dict) or &#39;model&#39; not in data or &#39;likelihood&#39; not in data or &#39;params&#39; not in data:
                raise Exception(&#39;parameter file has bad format&#39;)
            if not isinstance(data[&#39;params&#39;], list) or not all(isinstance(param, dict) for param in data[&#39;params&#39;]):
                raise Exception(&#39;parameter file has bad format&#39;)

            if data[&#39;model&#39;] != self.__class__.__name__:
                raise Exception(&#34;parameter file uses model &#39;%s&#39; which is different from current model &#39;%s&#39;&#34; % (data[&#39;model&#39;], self.__class__.__name__))

            cur_params = self.get_params()
            if len(data[&#39;params&#39;]) != len(cur_params):
                raise Exception(&#34;parameter file uses model with %d kernels which is different from current model that uses %d kernels, is the model&#39;s Q different?&#34; % (len(data[&#39;params&#39;]), len(cur_params)))

            for key, val in data[&#39;likelihood&#39;].items():
                self.set_likelihood_param(key, val)

            for q, param in enumerate(data[&#39;params&#39;]):
                for key, val in param.items():
                    self.set_param(q, key, val)

    def train(
        self,
        method=&#39;L-BFGS-B&#39;,
        tol=1e-6,
        maxiter=2000,
        opt_params={},
        params={},
        export_graph=False):
        &#34;&#34;&#34;
        Trains the model using the kernel and its parameters.

        For different optimizers, see scipy.optimize.minimize.
        It can be bounded by a maximum number of iterations, disp will output final
        optimization information. When using the &#39;Adam&#39; optimizer, a
        learning_rate can be set.

        Args:
            method (str): Optimizer to use, if &#34;Adam&#34; is chosen,
                gpflow.training.Adamoptimizer will be used, otherwise the passed scipy
                optimizer is used. Defaults to scipy &#39;L-BFGS-B&#39;.
            tol (float): Tolerance for optimizer. Defaults to 1e-6.
            maxiter (int): Maximum number of iterations. Defaults to 2000.
            opt_params (dict): Aditional dictionary with parameters on optimizer.
                If method is &#39;Adam&#39; see:
                https://github.com/GPflow/GPflow/blob/develop/gpflow/training/tensorflow_optimizer.py
                If method is in scipy-optimizer see:
                https://github.com/GPflow/GPflow/blob/develop/gpflow/training/scipy_optimizer.py
                https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html
            params (dict): Additional dictionary with parameters to minimize. 
                See https://github.com/GPflow/GPflow/blob/develop/gpflow/training/optimizer.py
                for more details.
            export_graph (bool): Default to False.

        Examples:
            &gt;&gt;&gt; model.train(tol=1e-6, maxiter=1e5)
            
            &gt;&gt;&gt; model.train(method=&#39;Adam&#39;, opt_params={...})
        &#34;&#34;&#34;
        start_time = time.time()
        with self.graph.as_default():
            with self.session.as_default():
                if export_graph:
                    def get_tensor(name):
                        return self.graph.get_tensor_by_name(&#39;GPR-&#39; + self.model._index + &#39;/likelihood_1/&#39; + name + &#39;:0&#39;)
                    writer = tf.summary.FileWriter(&#34;log&#34;, self.graph)
                    K_summary = tf.summary.histogram(&#39;K&#39;, get_tensor(&#39;K&#39;))

                step_i = 0
                def step(theta):
                    nonlocal step_i
                    if export_graph:
                        writer.add_summary(self.session.run(K_summary), step_i)
                    step_i += 1

                if method == &#34;Adam&#34;:
                    opt = gpflow.training.AdamOptimizer(**opt_params)
                    opt.minimize(self.model, anchor=True, **params)
                else:
                    opt = gpflow.train.ScipyOptimizer(method=method, tol=tol, **opt_params)
                    opt.minimize(self.model, anchor=True, step_callback=step, maxiter=maxiter, disp=True, **params)

        print(&#34;Done in %.1f minutes&#34; % ((time.time() - start_time)/60))

    ################################################################################
    # Predictions ##################################################################
    ################################################################################

    def predict(self, x=None, plot=False):
        &#34;&#34;&#34;
        Predict with model.

        Will make a prediction using x as input. If no input value is passed, the prediction will 
        be made with atribute self.X_pred that can be setted with other functions.
        It returns the X, Y_mu, Y_var values per channel.

        Args:
            x_pred (list, dict): Dictionary where keys are channel index and elements numpy arrays with channel inputs.

        Examples:
            &gt;&gt;&gt; model.predict(plot=True)
        &#34;&#34;&#34;
        if x is not None:
            self.dataset.set_pred(self.name, x)

        x = self.dataset.to_kernel_pred()
        if len(x) == 0:
                raise Exception(&#39;no prediction x range set, use pred_x argument or set manually using DataSet.set_pred() or Data.set_pred()&#39;)
        with self.graph.as_default():
            with self.session.as_default():
                mu, var = self.model.predict_f(x)
        self.dataset.from_kernel_pred(self.name, mu, var)
        
        if plot:
            self.plot_prediction()

    # TODO
    def plot_prediction(self, grid=None, figsize=(12, 8), ylims=None, names=None, title=&#39;&#39;, ret_fig=False):

        &#34;&#34;&#34;
        Plot training points, all data and prediction for training range for all channels.

        Args:
            grid (tuple) : Tuple with the 2 dimensions of the grid.
            figsize(tuple): Figure size, default to (12, 8).
            ylims(list): List of tuples with limits for Y axis for
                each channel.
            Names(list): List of the names of each title.
            title(str): Title of the plot.
            ret_fig(bool): If true returns the matplotlib figure, 
                array of axis and dictionary with all the points used.

        TODO: Add case for single output SM kernel.
        &#34;&#34;&#34;
        # get data
        x_train, y_train = self.dataset.get_data()
        x_all, y_all = self.dataset.get_all()
        x_pred, mu, lower, upper = self.dataset.get_pred(self.name)

        n_dim = self.dataset.get_output_dims()
        if n_dim == 1:
            grid = (1, 1)
        elif grid is None:
            grid = (int(np.ceil(n_dim/2)), 2)

        if (grid[0] * grid[1]) &lt; n_dim:
            raise Exception(&#39;grid not big enough for all channels&#39;)

        fig, axes = plt.subplots(grid[0], grid[1], sharex=False, figsize=figsize)
        axes = np.array(axes).reshape(-1)

        colors = list(matplotlib.colors.TABLEAU_COLORS)
        for i in range(n_dim):
            axes[i].plot(x_train[i][:,0], y_train[i], &#39;.k&#39;, label=&#39;Train&#39;, ms=5)
            axes[i].plot(x_all[i][:,0], y_all[i], &#39;--&#39;, label=&#39;Test&#39;, c=&#39;gray&#39;,lw=1.4, zorder=5)
            
            axes[i].plot(x_pred[i][:,0], mu[i], label=&#39;Post.Mean&#39;, c=colors[i], zorder=1)
            axes[i].fill_between(x_pred[i][:,0].reshape(-1),
                lower[i],
                upper[i],
                label=&#39;95% c.i&#39;,
                color=colors[i],
                alpha=0.4)
            
            axes[i].xaxis.set_major_locator(plt.MaxNLocator(6))

            formatter = matplotlib.ticker.FuncFormatter(lambda x,pos: self.dataset.get(i).formatters[0]._format(x))
            axes[i].xaxis.set_major_formatter(formatter)

            # set channels name
            if names is not None:
                axes[i].set_title(names[i])
            else:
                channel_name = self.dataset.get_names()[i]
                if channel_name != &#39;&#39;:
                    axes[i].set_title(channel_name)
                elif n_dim == 1:
                    pass
                else:
                    axes[i].set_title(&#39;Channel &#39; + str(i))

            # set y lims
            if ylims is not None:
                axes[i].set_ylim(ylims[i]) 
            
        plt.suptitle(title, y=1.02)
        plt.tight_layout()

        data_dict = {
            &#39;x_train&#39;:x_train,
            &#39;y_train&#39;:y_train,
            &#39;x_all&#39;:x_all,
            &#39;y_all&#39;:y_all,
            &#39;y_pred&#39;:mu,
            &#39;low_ci&#39;:lower,
            &#39;hi_ci&#39;:upper,
        }

        if ret_fig:
            return fig, axes, data_dict</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mogptk.sm.SM" href="sm.html#mogptk.sm.SM">SM</a></li>
<li><a title="mogptk.mosm.MOSM" href="mosm.html#mogptk.mosm.MOSM">MOSM</a></li>
<li><a title="mogptk.csm.CSM" href="csm.html#mogptk.csm.CSM">CSM</a></li>
<li><a title="mogptk.sm_lmc.SM_LMC" href="sm_lmc.html#mogptk.sm_lmc.SM_LMC">SM_LMC</a></li>
<li><a title="mogptk.conv.CONV" href="conv.html#mogptk.conv.CONV">CONV</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mogptk.model.model.fix_param"><code class="name flex">
<span>def <span class="ident">fix_param</span></span>(<span>self, key)</span>
</code></dt>
<dd>
<section class="desc"><p>Make parameter untrainable (undo with <code>unfix_param</code>).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>key</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the parameter.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; model.fix_param('variance')
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fix_param(self, key):
    &#34;&#34;&#34;
    Make parameter untrainable (undo with `unfix_param`).

    Args:
        key (str): Name of the parameter.

    Examples:
        &gt;&gt;&gt; model.fix_param(&#39;variance&#39;)
    &#34;&#34;&#34;
    if hasattr(self.model.kern, &#39;kernels&#39;):
        for kernel_i, kernel in enumerate(self.model.kern.kernels):
            for param_name, param_val in kernel.__dict__.items():
                if param_name == key and isinstance(param_val, gpflow.params.parameter.Parameter):
                    getattr(self.model.kern.kernels[kernel_i], param_name).trainable = False
    else:
        for param_name, param_val in self.model.kern.__dict__.items():
            if param_name == key and isinstance(param_val, gpflow.params.parameter.Parameter):
                getattr(self.model.kern, param_name).trainable = False</code></pre>
</details>
</dd>
<dt id="mogptk.model.model.get_likelihood_params"><code class="name flex">
<span>def <span class="ident">get_likelihood_params</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns all parameters set for the likelihood.</p>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; params = model.get_likelihood_params()
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_likelihood_params(self):
    &#34;&#34;&#34;
    Returns all parameters set for the likelihood.

    Examples:
        &gt;&gt;&gt; params = model.get_likelihood_params()
    &#34;&#34;&#34;
    params = {}
    for param_name, param_val in self.model.likelihood.__dict__.items():
        if isinstance(param_val, gpflow.params.parameter.Parameter):
            params[param_name] = param_val.read_value()
    return params</code></pre>
</details>
</dd>
<dt id="mogptk.model.model.get_param"><code class="name flex">
<span>def <span class="ident">get_param</span></span>(<span>self, q, key)</span>
</code></dt>
<dd>
<section class="desc"><p>Gets a kernel parameter for component 'q' with key the parameter name.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>q</code></strong> :&ensp;<code>int</code></dt>
<dd>Component of kernel.</dd>
<dt><strong><code>key</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of component.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>val</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Value of parameter.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; val = model.get_param(0, 'variance') # for Q=0 get the parameter called 'variance'
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_param(self, q, key):
    &#34;&#34;&#34;
    Gets a kernel parameter for component &#39;q&#39; with key the parameter name.

    Args:
        q (int): Component of kernel.
        key (str): Name of component.
        
    Returns:
        val (numpy.ndarray): Value of parameter.

    Examples:
        &gt;&gt;&gt; val = model.get_param(0, &#39;variance&#39;) # for Q=0 get the parameter called &#39;variance&#39;
    &#34;&#34;&#34;
    if hasattr(self.model.kern, &#39;kernels&#39;):
        if q &lt; 0 or len(self.model.kern.kernels) &lt;= q:
            raise Exception(&#34;qth component %d does not exist&#34; % (q))
        kern = self.model.kern.kernels[q].__dict__
    else:
        if q != 0:
            raise Exception(&#34;qth component %d does not exist&#34; % (q))
        kern = self.model.kern.__dict__
    
    if key not in kern or not isinstance(kern[key], gpflow.params.parameter.Parameter):
        raise Exception(&#34;parameter name &#39;%s&#39; does not exist&#34; % (key))

    return kern[key].read_value()</code></pre>
</details>
</dd>
<dt id="mogptk.model.model.get_params"><code class="name flex">
<span>def <span class="ident">get_params</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns all parameters set for the kernel per component.</p>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; params = model.get_params()
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_params(self):
    &#34;&#34;&#34;
    Returns all parameters set for the kernel per component.

    Examples:
        &gt;&gt;&gt; params = model.get_params()
    &#34;&#34;&#34;
    params = []
    if hasattr(self.model.kern, &#39;kernels&#39;):
        for kernel_i, kernel in enumerate(self.model.kern.kernels):
            params.append({})
            for param_name, param_val in kernel.__dict__.items():
                if isinstance(param_val, gpflow.params.parameter.Parameter):
                    params[kernel_i][param_name] = param_val.read_value()
    else:
        params.append({})
        for param_name, param_val in self.model.kern.__dict__.items():
            if isinstance(param_val, gpflow.params.parameter.Parameter):
                params[0][param_name] = param_val.read_value()
    return params</code></pre>
</details>
</dd>
<dt id="mogptk.model.model.load_params"><code class="name flex">
<span>def <span class="ident">load_params</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<section class="desc"><p>Load model parameters from a given file that was previously saved with <code>save_params()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename to load from, automatically appends '.params'.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; model.load_params('filename')
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_params(self, filename):
    &#34;&#34;&#34;
    Load model parameters from a given file that was previously saved with `save_params()`.

    Args:
        filename (str): Filename to load from, automatically appends &#39;.params&#39;.

    Examples:
        &gt;&gt;&gt; model.load_params(&#39;filename&#39;)
    &#34;&#34;&#34;
    filename += &#34;.&#34; + self.name + &#34;.params&#34;

    with open(filename) as r:
        data = json.load(r)

        if not isinstance(data, dict) or &#39;model&#39; not in data or &#39;likelihood&#39; not in data or &#39;params&#39; not in data:
            raise Exception(&#39;parameter file has bad format&#39;)
        if not isinstance(data[&#39;params&#39;], list) or not all(isinstance(param, dict) for param in data[&#39;params&#39;]):
            raise Exception(&#39;parameter file has bad format&#39;)

        if data[&#39;model&#39;] != self.__class__.__name__:
            raise Exception(&#34;parameter file uses model &#39;%s&#39; which is different from current model &#39;%s&#39;&#34; % (data[&#39;model&#39;], self.__class__.__name__))

        cur_params = self.get_params()
        if len(data[&#39;params&#39;]) != len(cur_params):
            raise Exception(&#34;parameter file uses model with %d kernels which is different from current model that uses %d kernels, is the model&#39;s Q different?&#34; % (len(data[&#39;params&#39;]), len(cur_params)))

        for key, val in data[&#39;likelihood&#39;].items():
            self.set_likelihood_param(key, val)

        for q, param in enumerate(data[&#39;params&#39;]):
            for key, val in param.items():
                self.set_param(q, key, val)</code></pre>
</details>
</dd>
<dt id="mogptk.model.model.plot_prediction"><code class="name flex">
<span>def <span class="ident">plot_prediction</span></span>(<span>self, grid=None, figsize=(12, 8), ylims=None, names=None, title='', ret_fig=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Plot training points, all data and prediction for training range for all channels.</p>
<h2 id="args">Args</h2>
<p>grid (tuple) : Tuple with the 2 dimensions of the grid.
figsize(tuple): Figure size, default to (12, 8).
ylims(list): List of tuples with limits for Y axis for
each channel.
Names(list): List of the names of each title.
title(str): Title of the plot.
ret_fig(bool): If true returns the matplotlib figure,
array of axis and dictionary with all the points used.
TODO: Add case for single output SM kernel.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plot_prediction(self, grid=None, figsize=(12, 8), ylims=None, names=None, title=&#39;&#39;, ret_fig=False):

    &#34;&#34;&#34;
    Plot training points, all data and prediction for training range for all channels.

    Args:
        grid (tuple) : Tuple with the 2 dimensions of the grid.
        figsize(tuple): Figure size, default to (12, 8).
        ylims(list): List of tuples with limits for Y axis for
            each channel.
        Names(list): List of the names of each title.
        title(str): Title of the plot.
        ret_fig(bool): If true returns the matplotlib figure, 
            array of axis and dictionary with all the points used.

    TODO: Add case for single output SM kernel.
    &#34;&#34;&#34;
    # get data
    x_train, y_train = self.dataset.get_data()
    x_all, y_all = self.dataset.get_all()
    x_pred, mu, lower, upper = self.dataset.get_pred(self.name)

    n_dim = self.dataset.get_output_dims()
    if n_dim == 1:
        grid = (1, 1)
    elif grid is None:
        grid = (int(np.ceil(n_dim/2)), 2)

    if (grid[0] * grid[1]) &lt; n_dim:
        raise Exception(&#39;grid not big enough for all channels&#39;)

    fig, axes = plt.subplots(grid[0], grid[1], sharex=False, figsize=figsize)
    axes = np.array(axes).reshape(-1)

    colors = list(matplotlib.colors.TABLEAU_COLORS)
    for i in range(n_dim):
        axes[i].plot(x_train[i][:,0], y_train[i], &#39;.k&#39;, label=&#39;Train&#39;, ms=5)
        axes[i].plot(x_all[i][:,0], y_all[i], &#39;--&#39;, label=&#39;Test&#39;, c=&#39;gray&#39;,lw=1.4, zorder=5)
        
        axes[i].plot(x_pred[i][:,0], mu[i], label=&#39;Post.Mean&#39;, c=colors[i], zorder=1)
        axes[i].fill_between(x_pred[i][:,0].reshape(-1),
            lower[i],
            upper[i],
            label=&#39;95% c.i&#39;,
            color=colors[i],
            alpha=0.4)
        
        axes[i].xaxis.set_major_locator(plt.MaxNLocator(6))

        formatter = matplotlib.ticker.FuncFormatter(lambda x,pos: self.dataset.get(i).formatters[0]._format(x))
        axes[i].xaxis.set_major_formatter(formatter)

        # set channels name
        if names is not None:
            axes[i].set_title(names[i])
        else:
            channel_name = self.dataset.get_names()[i]
            if channel_name != &#39;&#39;:
                axes[i].set_title(channel_name)
            elif n_dim == 1:
                pass
            else:
                axes[i].set_title(&#39;Channel &#39; + str(i))

        # set y lims
        if ylims is not None:
            axes[i].set_ylim(ylims[i]) 
        
    plt.suptitle(title, y=1.02)
    plt.tight_layout()

    data_dict = {
        &#39;x_train&#39;:x_train,
        &#39;y_train&#39;:y_train,
        &#39;x_all&#39;:x_all,
        &#39;y_all&#39;:y_all,
        &#39;y_pred&#39;:mu,
        &#39;low_ci&#39;:lower,
        &#39;hi_ci&#39;:upper,
    }

    if ret_fig:
        return fig, axes, data_dict</code></pre>
</details>
</dd>
<dt id="mogptk.model.model.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x=None, plot=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Predict with model.</p>
<p>Will make a prediction using x as input. If no input value is passed, the prediction will
be made with atribute self.X_pred that can be setted with other functions.
It returns the X, Y_mu, Y_var values per channel.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x_pred</code></strong> :&ensp;<code>list</code>, <code>dict</code></dt>
<dd>Dictionary where keys are channel index and elements numpy arrays with channel inputs.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; model.predict(plot=True)
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def predict(self, x=None, plot=False):
    &#34;&#34;&#34;
    Predict with model.

    Will make a prediction using x as input. If no input value is passed, the prediction will 
    be made with atribute self.X_pred that can be setted with other functions.
    It returns the X, Y_mu, Y_var values per channel.

    Args:
        x_pred (list, dict): Dictionary where keys are channel index and elements numpy arrays with channel inputs.

    Examples:
        &gt;&gt;&gt; model.predict(plot=True)
    &#34;&#34;&#34;
    if x is not None:
        self.dataset.set_pred(self.name, x)

    x = self.dataset.to_kernel_pred()
    if len(x) == 0:
            raise Exception(&#39;no prediction x range set, use pred_x argument or set manually using DataSet.set_pred() or Data.set_pred()&#39;)
    with self.graph.as_default():
        with self.session.as_default():
            mu, var = self.model.predict_f(x)
    self.dataset.from_kernel_pred(self.name, mu, var)
    
    if plot:
        self.plot_prediction()</code></pre>
</details>
</dd>
<dt id="mogptk.model.model.print_params"><code class="name flex">
<span>def <span class="ident">print_params</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Print the parameters of the model in a table.</p>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; model.print_params()
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def print_params(self):
    &#34;&#34;&#34;
    Print the parameters of the model in a table.

    Examples:
        &gt;&gt;&gt; model.print_params()
    &#34;&#34;&#34;
    with np.printoptions(precision=3, floatmode=&#39;fixed&#39;):
        table = &#39;&#39;
        for q, params in enumerate(self.get_params()):
            contents = []
            output_dims = self.dataset.get_output_dims()
            for j in range(output_dims):
                content = &#39;&lt;td&gt;%s&lt;/td&gt;&#39; % (self.dataset[j].get_name(),)

                values = []
                for key in params.keys():
                    val = params[key]
                    if val.ndim == 1 and val.shape[0] == output_dims:
                        values.append(&#39;%.3f&#39; % (val[j],))
                    elif val.ndim == 2 and val.shape[1] == output_dims:
                        values.append(str(val[:,j]))
                    else:
                        values.append(str(val))
                content += &#39;&lt;td&gt;&#39; + &#39;&lt;/td&gt;&lt;td&gt;&#39;.join(values) + &#39;&lt;/td&gt;&#39;
                contents.append(content)

            header = &#39;&lt;tr&gt;&lt;th&gt;Q=%d&lt;/th&gt;&lt;th&gt;&#39; % (q) + &#39;&lt;/th&gt;&lt;th&gt;&#39;.join(params.keys()) + &#39;&lt;/th&gt;&lt;/tr&gt;&#39;
            contents = &#39;&lt;tr&gt;&#39; + &#39;&lt;/tr&gt;&lt;tr&gt;&#39;.join(contents) + &#39;&lt;/tr&gt;&#39;
            table += header + contents
        display(HTML(&#39;&lt;table&gt;%s&lt;/table&gt;&#39; % (table)))

        likelihood = self.get_likelihood_params()
        table = &#39;&lt;tr&gt;&lt;th&gt;Likelihood&lt;/th&gt;&#39;
        for key in likelihood:
            table += &#39;&lt;th&gt;%s&lt;/th&gt;&#39; % (key,)
        table += &#39;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&#39;
        for key in likelihood:
            table += &#39;&lt;td&gt;%s&lt;/td&gt;&#39; % (likelihood[key],)
        table += &#39;&lt;/tr&gt;&#39;
        display(HTML(&#39;&lt;table&gt;%s&lt;/table&gt;&#39; % (table)))</code></pre>
</details>
</dd>
<dt id="mogptk.model.model.save_params"><code class="name flex">
<span>def <span class="ident">save_params</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<section class="desc"><p>Save model parameters to a given file that can then be loaded with <code>load_params()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename to save to, automatically appends '.params'.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; model.save_params('filename')
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_params(self, filename):
    &#34;&#34;&#34;
    Save model parameters to a given file that can then be loaded with `load_params()`.

    Args:
        filename (str): Filename to save to, automatically appends &#39;.params&#39;.

    Examples:
        &gt;&gt;&gt; model.save_params(&#39;filename&#39;)
    &#34;&#34;&#34;
    filename += &#34;.&#34; + self.name + &#34;.params&#34;

    try:
        os.remove(filename)
    except OSError:
        pass
    
    class NumpyEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            return json.JSONEncoder.default(self, obj)

    data = {
        &#39;model&#39;: self.__class__.__name__,
        &#39;likelihood&#39;: self.get_likelihood_params(),
        &#39;params&#39;: self.get_params()
    }
    with open(filename, &#39;w&#39;) as w:
        json.dump(data, w, cls=NumpyEncoder)</code></pre>
</details>
</dd>
<dt id="mogptk.model.model.set_likelihood_param"><code class="name flex">
<span>def <span class="ident">set_likelihood_param</span></span>(<span>self, key, val)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets a likelihood parameter with key the parameter name.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>key</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of component.</dd>
<dt><strong><code>val</code></strong> :&ensp;<code>float</code>, <code>ndarray</code></dt>
<dd>Value of parameter.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; model.set_likelihood_param('variance', np.array([5.0, 3.0])) # set the parameter called 'variance'
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_likelihood_param(self, key, val):
    &#34;&#34;&#34;
    Sets a likelihood parameter with key the parameter name.

    Args:
        key (str): Name of component.
        val (float, ndarray): Value of parameter.

    Examples:
        &gt;&gt;&gt; model.set_likelihood_param(&#39;variance&#39;, np.array([5.0, 3.0])) # set the parameter called &#39;variance&#39;
    &#34;&#34;&#34;
    if isinstance(val, (int, float, list)):
        val = np.array(val)
    if not isinstance(val, np.ndarray):
        raise Exception(&#34;value %s of type %s is not a number type or ndarray&#34; % (val, type(val)))

    likelihood = self.model.likelihood.__dict__
    if key not in likelihood or not isinstance(likelihood[key], gpflow.params.parameter.Parameter):
        raise Exception(&#34;parameter name &#39;%s&#39; does not exist&#34; % (key))

    if likelihood[key].shape != val.shape:
        raise Exception(&#34;parameter name &#39;%s&#39; must have shape %s and not %s&#34; % (key, likelihood[key].shape, val.shape))

    with self.graph.as_default():
        with self.session.as_default():
            likelihood[key].assign(val)</code></pre>
</details>
</dd>
<dt id="mogptk.model.model.set_param"><code class="name flex">
<span>def <span class="ident">set_param</span></span>(<span>self, q, key, val)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets a kernel parameter for component 'q' with key the parameter name.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>q</code></strong> :&ensp;<code>int</code></dt>
<dd>Component of kernel.</dd>
<dt><strong><code>key</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of component.</dd>
<dt><strong><code>val</code></strong> :&ensp;<code>float</code>, <code>numpy.ndarray</code></dt>
<dd>Value of parameter.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; model.set_param(0, 'variance', np.array([5.0, 3.0])) # for Q=0 set the parameter called 'variance'
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_param(self, q, key, val):
    &#34;&#34;&#34;
    Sets a kernel parameter for component &#39;q&#39; with key the parameter name.

    Args:
        q (int): Component of kernel.
        key (str): Name of component.
        val (float, numpy.ndarray): Value of parameter.

    Examples:
        &gt;&gt;&gt; model.set_param(0, &#39;variance&#39;, np.array([5.0, 3.0])) # for Q=0 set the parameter called &#39;variance&#39;
    &#34;&#34;&#34;
    if isinstance(val, (int, float, list)):
        val = np.array(val)
    if not isinstance(val, np.ndarray):
        raise Exception(&#34;value %s of type %s is not a number type or ndarray&#34; % (val, type(val)))

    if hasattr(self.model.kern, &#39;kernels&#39;):
        if q &lt; 0 or len(self.model.kern.kernels) &lt;= q:
            raise Exception(&#34;qth component %d does not exist&#34; % (q))
        kern = self.model.kern.kernels[q].__dict__
    else:
        if q != 0:
            raise Exception(&#34;qth component %d does not exist&#34; % (q))
        kern = self.model.kern.__dict__

    if key not in kern or not isinstance(kern[key], gpflow.params.parameter.Parameter):
        raise Exception(&#34;parameter name &#39;%s&#39; does not exist&#34; % (key))

    if kern[key].shape != val.shape:
        raise Exception(&#34;parameter name &#39;%s&#39; must have shape %s and not %s&#34; % (key, kern[key].shape, val.shape))

    with self.graph.as_default():
        with self.session.as_default():
            kern[key].assign(val)</code></pre>
</details>
</dd>
<dt id="mogptk.model.model.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, method='L-BFGS-B', tol=1e-06, maxiter=2000, opt_params={}, params={}, export_graph=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Trains the model using the kernel and its parameters.</p>
<p>For different optimizers, see scipy.optimize.minimize.
It can be bounded by a maximum number of iterations, disp will output final
optimization information. When using the 'Adam' optimizer, a
learning_rate can be set.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>method</code></strong> :&ensp;<code>str</code></dt>
<dd>Optimizer to use, if "Adam" is chosen,
gpflow.training.Adamoptimizer will be used, otherwise the passed scipy
optimizer is used. Defaults to scipy 'L-BFGS-B'.</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code></dt>
<dd>Tolerance for optimizer. Defaults to 1e-6.</dd>
<dt><strong><code>maxiter</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum number of iterations. Defaults to 2000.</dd>
<dt><strong><code>opt_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Aditional dictionary with parameters on optimizer.
If method is 'Adam' see:
<a href="https://github.com/GPflow/GPflow/blob/develop/gpflow/training/tensorflow_optimizer.py">https://github.com/GPflow/GPflow/blob/develop/gpflow/training/tensorflow_optimizer.py</a>
If method is in scipy-optimizer see:
<a href="https://github.com/GPflow/GPflow/blob/develop/gpflow/training/scipy_optimizer.py">https://github.com/GPflow/GPflow/blob/develop/gpflow/training/scipy_optimizer.py</a>
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html</a></dd>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Additional dictionary with parameters to minimize.
See <a href="https://github.com/GPflow/GPflow/blob/develop/gpflow/training/optimizer.py">https://github.com/GPflow/GPflow/blob/develop/gpflow/training/optimizer.py</a>
for more details.</dd>
<dt><strong><code>export_graph</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default to False.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; model.train(tol=1e-6, maxiter=1e5)

&gt;&gt;&gt; model.train(method='Adam', opt_params={...})
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def train(
    self,
    method=&#39;L-BFGS-B&#39;,
    tol=1e-6,
    maxiter=2000,
    opt_params={},
    params={},
    export_graph=False):
    &#34;&#34;&#34;
    Trains the model using the kernel and its parameters.

    For different optimizers, see scipy.optimize.minimize.
    It can be bounded by a maximum number of iterations, disp will output final
    optimization information. When using the &#39;Adam&#39; optimizer, a
    learning_rate can be set.

    Args:
        method (str): Optimizer to use, if &#34;Adam&#34; is chosen,
            gpflow.training.Adamoptimizer will be used, otherwise the passed scipy
            optimizer is used. Defaults to scipy &#39;L-BFGS-B&#39;.
        tol (float): Tolerance for optimizer. Defaults to 1e-6.
        maxiter (int): Maximum number of iterations. Defaults to 2000.
        opt_params (dict): Aditional dictionary with parameters on optimizer.
            If method is &#39;Adam&#39; see:
            https://github.com/GPflow/GPflow/blob/develop/gpflow/training/tensorflow_optimizer.py
            If method is in scipy-optimizer see:
            https://github.com/GPflow/GPflow/blob/develop/gpflow/training/scipy_optimizer.py
            https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html
        params (dict): Additional dictionary with parameters to minimize. 
            See https://github.com/GPflow/GPflow/blob/develop/gpflow/training/optimizer.py
            for more details.
        export_graph (bool): Default to False.

    Examples:
        &gt;&gt;&gt; model.train(tol=1e-6, maxiter=1e5)
        
        &gt;&gt;&gt; model.train(method=&#39;Adam&#39;, opt_params={...})
    &#34;&#34;&#34;
    start_time = time.time()
    with self.graph.as_default():
        with self.session.as_default():
            if export_graph:
                def get_tensor(name):
                    return self.graph.get_tensor_by_name(&#39;GPR-&#39; + self.model._index + &#39;/likelihood_1/&#39; + name + &#39;:0&#39;)
                writer = tf.summary.FileWriter(&#34;log&#34;, self.graph)
                K_summary = tf.summary.histogram(&#39;K&#39;, get_tensor(&#39;K&#39;))

            step_i = 0
            def step(theta):
                nonlocal step_i
                if export_graph:
                    writer.add_summary(self.session.run(K_summary), step_i)
                step_i += 1

            if method == &#34;Adam&#34;:
                opt = gpflow.training.AdamOptimizer(**opt_params)
                opt.minimize(self.model, anchor=True, **params)
            else:
                opt = gpflow.train.ScipyOptimizer(method=method, tol=tol, **opt_params)
                opt.minimize(self.model, anchor=True, step_callback=step, maxiter=maxiter, disp=True, **params)

    print(&#34;Done in %.1f minutes&#34; % ((time.time() - start_time)/60))</code></pre>
</details>
</dd>
<dt id="mogptk.model.model.unfix_param"><code class="name flex">
<span>def <span class="ident">unfix_param</span></span>(<span>self, key)</span>
</code></dt>
<dd>
<section class="desc"><p>Make parameter trainable (that was previously fixed, see <code>fix_param</code>).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>key</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the parameter.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code>&gt;&gt;&gt; model.unfix_param('variance')
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def unfix_param(self, key):
    &#34;&#34;&#34;
    Make parameter trainable (that was previously fixed, see `fix_param`).

    Args:
        key (str): Name of the parameter.

    Examples:
        &gt;&gt;&gt; model.unfix_param(&#39;variance&#39;)
    &#34;&#34;&#34;
    if hasattr(self.model.kern, &#39;kernels&#39;):
        for kernel_i, kernel in enumerate(self.model.kern.kernels):
            for param_name, param_val in kernel.__dict__.items():
                if param_name == key and isinstance(param_val, gpflow.params.parameter.Parameter):
                    getattr(self.model.kern.kernels[kernel_i], param_name).trainable = True
    else:
        for param_name, param_val in self.model.kern.__dict__.items():
            if param_name == key and isinstance(param_val, gpflow.params.parameter.Parameter):
                getattr(self.model.kern, param_name).trainable = True</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mogptk" href="index.html">mogptk</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mogptk.model.model" href="#mogptk.model.model">model</a></code></h4>
<ul class="">
<li><code><a title="mogptk.model.model.fix_param" href="#mogptk.model.model.fix_param">fix_param</a></code></li>
<li><code><a title="mogptk.model.model.get_likelihood_params" href="#mogptk.model.model.get_likelihood_params">get_likelihood_params</a></code></li>
<li><code><a title="mogptk.model.model.get_param" href="#mogptk.model.model.get_param">get_param</a></code></li>
<li><code><a title="mogptk.model.model.get_params" href="#mogptk.model.model.get_params">get_params</a></code></li>
<li><code><a title="mogptk.model.model.load_params" href="#mogptk.model.model.load_params">load_params</a></code></li>
<li><code><a title="mogptk.model.model.plot_prediction" href="#mogptk.model.model.plot_prediction">plot_prediction</a></code></li>
<li><code><a title="mogptk.model.model.predict" href="#mogptk.model.model.predict">predict</a></code></li>
<li><code><a title="mogptk.model.model.print_params" href="#mogptk.model.model.print_params">print_params</a></code></li>
<li><code><a title="mogptk.model.model.save_params" href="#mogptk.model.model.save_params">save_params</a></code></li>
<li><code><a title="mogptk.model.model.set_likelihood_param" href="#mogptk.model.model.set_likelihood_param">set_likelihood_param</a></code></li>
<li><code><a title="mogptk.model.model.set_param" href="#mogptk.model.model.set_param">set_param</a></code></li>
<li><code><a title="mogptk.model.model.train" href="#mogptk.model.model.train">train</a></code></li>
<li><code><a title="mogptk.model.model.unfix_param" href="#mogptk.model.model.unfix_param">unfix_param</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>