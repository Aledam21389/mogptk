{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following example runs over a dataset of aerobic actions recorded from subjects \"using the Inertial Measurement Unit (IMU) on an Apple iPhone 4 smartphone. The IMU includes a 3D accelerometer, gyroscope, and magnetometer*. Each sample was taken at 60Hz, and manually trimmed to 500 samples (8.33s) to eliminate starting and stopping movements. iPhone is always clipped to the belt on the right hand side.\"\n",
    "\n",
    "Each file contains 500 rows, each row with the following information:\n",
    "Acc_x,Acc_y,Acc_z,Gyr_x,Gyr_y,Gyr_z,Mag_x,Mag_y,Mag_z\n",
    "\n",
    "Each sensor has 3 channels.\n",
    "\n",
    "You may find the dataset and revelant information about the publication 'Corey McCall, Kishore Reddy and Mubarak Shah, Macro-Class Selection for Hierarchical K-NN Classification of Inertial Sensor Data, Second International Conference on Pervasive and Embedded Computing and Communication Systems, PECCS 2012, February 24-26, 2012, Rome, Italy.' at: http://crcv.ucf.edu/data/UCF-iPhone.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from os import listdir\n",
    "import time\n",
    "import mogptk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions transform the dataset info into a more readily usable format, for multi-output regression. Note that the original files only contain the y-axis information, in increasing time order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the class of the activity is encoded in the filename we use this information to create numerical labels.\n",
    "def conversion(label):\n",
    "    if(label == 'bike'):\n",
    "        return 0\n",
    "    elif(label == 'climbing'):\n",
    "        return 1\n",
    "    elif(label == 'descending'):\n",
    "        return 2\n",
    "    elif(label == 'gymbike'):\n",
    "        return 3\n",
    "    elif(label == 'jumping'):\n",
    "        return 4\n",
    "    elif(label == 'running'):\n",
    "        return 5\n",
    "    elif(label == 'standing'):\n",
    "        return 6\n",
    "    elif(label == 'treadmill'):\n",
    "        return 7\n",
    "    elif(label == 'walking'):\n",
    "        return 8\n",
    "    \n",
    "#For each one of the folders we get all the filenames within them.\n",
    "def get_file_names(path):\n",
    "    folder = path + 'S0%d'\n",
    "    filenames = []\n",
    "    for x in range(1,10):\n",
    "        current_folder = folder % x\n",
    "        onlyfiles = [current_folder + \"/\" + f for f in listdir(current_folder)]\n",
    "        filenames.append(sorted(onlyfiles))\n",
    "    return filenames\n",
    "\n",
    "#Given the full_path (where the dataset resides in memory) we get all the filenames of all the dataset folders\n",
    "#and generate (Y,label) lists.\n",
    "def make_dataset(full_path):\n",
    "    full_file_names = get_file_names(full_path)\n",
    "    Y = []\n",
    "    label_names = []\n",
    "    label_numbers = []\n",
    "    for folder_number in range(9):\n",
    "        for filename in range(len(full_file_names[folder_number])):\n",
    "            path = full_file_names[folder_number][filename]\n",
    "            sample_data = np.genfromtxt(path, delimiter=',')\n",
    "            Y.append(sample_data)\n",
    "            label = path.split('/')\n",
    "            label = label[len(label)-1].split('.')[0]\n",
    "            label = label[0:len(label)-1]\n",
    "            label_names.append(label)\n",
    "            label_numbers.append(conversion(label))\n",
    "\n",
    "    return Y, label_numbers, label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can access the y-axis information for each channel we have to create an x-axis counterpart to feed the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = '../data/HAR/Smartphone_Dataset/'\n",
    "measurements, label_number, label_names = make_dataset(full_path)\n",
    "\n",
    "#Since we're fitting curves, instead of performing a classification, we won't be using\n",
    "#the class labels. We have to fabricate an X component to our y (the measurements).\n",
    "#The measurements correspond to 9 channels (3 per sensor: accel, gyro, magnetometer) at\n",
    "#a rate of 60hz. There's 500 measurements per channel, so the total time spanned is approx\n",
    "#8.33s\n",
    "X_list_bike = []\n",
    "Y_list_bike = []\n",
    "X_list_climb = []\n",
    "Y_list_climb = []\n",
    "#Note that measurements is a list containing all y-values for all channels for all experiments. So, by \n",
    "#accessing measurements[0] we are acquiring all the y-values for all 9 channels of experiment 0 which\n",
    "#happens to be a bicycle ride.\n",
    "measurements_for_one_bicycle_ride = measurements[0] #Remember that measurements is a list containing \n",
    "#Experiment 5 is an instance of climbing.\n",
    "measurements_for_one_instance_of_climbing = measurements[5]\n",
    "\n",
    "#The following loop allows us to pick any number of channels, without modifying channel order.\n",
    "number_of_channels = 9\n",
    "for index in range(number_of_channels):\n",
    "    X_list_bike.append(np.array([x/60 for x in range(500)]))\n",
    "    X_list_climb.append(np.array([x/60 for x in range(500)]))\n",
    "    #We also remove the mean from the y-values to better approximate a mean=0 GP.\n",
    "    Y_list_bike.append(measurements_for_one_bicycle_ride[:,index]-measurements_for_one_bicycle_ride[:,index].mean())\n",
    "    Y_list_climb.append(measurements_for_one_instance_of_climbing[:,index]-measurements_for_one_instance_of_climbing[:,index].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cbc80e72fe10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#We add the training data (in multi-output format) to the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#We create testing data (a high resolution set of equally spaced points in an interval).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/chile/GPs/MultiOutputGP-Toolbox/mogptk/model.py\u001b[0m in \u001b[0;36madd_training_data\u001b[0;34m(self, X, y, norm)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_lists_into_multioutput_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m#Transform to multioutput format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/chile/GPs/MultiOutputGP-Toolbox/mogptk/model.py\u001b[0m in \u001b[0;36mtransform_lists_into_multioutput_format\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mcurrent_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mcurrent_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mx_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "starting_time = time.time()\n",
    "#We create the model, with 4 components and L-BFGS-B as the optimizer.\n",
    "model = mogptk.model(4, optimizer = 'L-BFGS-B')\n",
    "#We'll consider the data pertaining the previously loaded climbing experiment (Experiment 5).\n",
    "X_original, Y_original = X_list_climb, Y_list_climb\n",
    "\n",
    "#To challenge the model in the reconstruction task we delete portions of the signals, randomly.\n",
    "X_new, Y_new, X_deleted, Y_deleted = model.remove_slabs(X_list_climb, Y_list_climb)\n",
    "\n",
    "#We transform the data to the multi-output format.\n",
    "X_input, Y_input = model.transform_lists_into_multioutput_format(X_new, Y_new)\n",
    "#For plotting purposes, we add the removed data. The last argument specifies the channel\n",
    "#to which the removed data belongs. Since we're adding the deleted data in the same order\n",
    "#as the original channels we just provide a range from 0 to number_of_channels.\n",
    "model.add_extra_observations(X_deleted, Y_deleted, [i for i in range(number_of_channels)])\n",
    "\n",
    "#We add the training data (in multi-output format) to the model.\n",
    "model.add_training_data(X_input,Y_input)\n",
    "\n",
    "#We create testing data (a high resolution set of equally spaced points in an interval).\n",
    "#First argument is the desired resolution (number of points between start and ending point), second argument are the desired channels to predict upon.\n",
    "#Since our samples are taken at 60hz and there's 500 of them we define a range from 0s to 8.4s.\n",
    "X_pred_new = model.predict_interval(1000, [x for x in range(number_of_channels)], start = [[0] for x in range(number_of_channels)],end = [[8.4] for x in range(number_of_channels)]) \n",
    "\n",
    "################## Build model and optimization #####################\n",
    "#In the example notebook we showed that the model can be built with randomly chosen \n",
    "#starting parameters and then optimized to minimize NLL.\n",
    "#In this case we will use one of the heuristics included in the toolbox.\n",
    "#What this heuristic does, briefly, is:\n",
    "#1) Find the most relevant frequencies in the training data using bayesian non-parametric spectral estimation.\n",
    "#2) Use the found frequencies to build the model (all other parameters taken randomly, where appropriate).\n",
    "#3) Optimize from there.\n",
    "model.optimization_heuristic_zero(iterations=1000)\n",
    "#####################################################################\n",
    "\n",
    "#We can compute mean absolute error over some data. Note that, in this example,\n",
    "#we'll be computing over all the original data points, which includes the removed\n",
    "#observations not used to optimize the model.\n",
    "mae = model.compute_mae(X_list_climb, Y_list_climb)\n",
    "print(\"MAE over original data = \", mae)\n",
    "\n",
    "#Perform the prediction over the test data.\n",
    "Y_pred, STD_pred = model.predict(X_pred_new)\n",
    "\n",
    "#We inspect model parameters, after they have been optimized.\n",
    "model.anchor_model()\n",
    "print(model.read_trainables())\n",
    "\n",
    "#We print the results of the prediction with and without variance.\n",
    "model.make_plots(\"smartphone_climbing.png\", var=True) #Variance is included in the plot.\n",
    "model.make_plots(\"smartphone_climbing_no_var\", var=False, title='Experiment with no variance') #Variance is not included.\n",
    "\n",
    "#We can save the optimized model to memory.\n",
    "model.save(\"smartphone_model_optimized\")\n",
    "end_time = time.time()\n",
    "print(\"Total time for climbing: \", end_time-starting_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Due to the polling rate of the instrument the data points do not always lead to smooth curves (e.g: channel 6 near the 2.7s mark or channel 8 at 7s), which, of course, hinders the model's performance.\n",
    "\n",
    "Let's see another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_y_from_measurements(measurement_list, no_of_channels):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    for index in range(no_of_channels):\n",
    "        X_list.append(np.array([x/60 for x in range(500)]))\n",
    "        y_list.append(measurement_list[:,index]-measurement_list[:,index].mean())\n",
    "    return X_list, y_list\n",
    "\n",
    "X_another_example, y_another_example = make_X_y_from_measurements(measurements[26], 9)\n",
    "\n",
    "starting_time = time.time()\n",
    "#We create the model, with 4 components and L-BFGS-B as the optimizer.\n",
    "another_model = mogp_model(4, optimizer = 'L-BFGS-B')\n",
    "\n",
    "X_another, y_another, X_deleted_another, y_deleted_another = another_model.remove_slabs(X_another_example, y_another_example)\n",
    "\n",
    "X_input_another, Y_input_another = another_model.transform_lists_into_multioutput_format(X_another, y_another)\n",
    "\n",
    "another_model.add_extra_observations(X_deleted_another, y_deleted_another, [i for i in range(number_of_channels)])\n",
    "\n",
    "#We add the training data (in multi-output format) to the model.\n",
    "another_model.add_training_data(X_input_another,Y_input_another)\n",
    "\n",
    "X_pred_new = another_model.predict_interval(1000, [x for x in range(number_of_channels)], start = [[0] for x in range(number_of_channels)],end = [[8.4] for x in range(number_of_channels)]) \n",
    "\n",
    "################## Build model and optimization #####################\n",
    "#In the example notebook we showed that the model can be built with randomly chosen \n",
    "#starting parameters and then optimized to minimize NLL.\n",
    "#In this case we will use one of the heuristics included in the toolbox.\n",
    "#What this heuristic does, briefly, is:\n",
    "#1) Find the most relevant frequencies in the training data using bayesian non-parametric spectral estimation.\n",
    "#2) Use the found frequencies to build the model (all other parameters taken randomly, where appropriate).\n",
    "#3) Optimize in cascading fashion, first optimizing variances and constants, then delays, phases and means, then delays variances and constants. \n",
    "#4) Finally there's a last round of optimization over all parameters.\n",
    "another_model.optimization_heuristic_one()\n",
    "#####################################################################\n",
    "\n",
    "#Perform the prediction over the test data.\n",
    "Y_pred, STD_pred = another_model.predict(X_pred_new)\n",
    "\n",
    "#We can compute mean absolute error over some data. Note that, in this example,\n",
    "#we'll be computing over all the original data points, which includes the removed\n",
    "#observations not used to optimize the model.\n",
    "mae = another_model.compute_mae(X_list_climb, y_list_climb)\n",
    "print(\"MAE over original data = \", mae)\n",
    "\n",
    "#We inspect model parameters, after they have been optimized.\n",
    "another_model.anchor_model()\n",
    "print(another_model.read_trainables())\n",
    "\n",
    "#We print the results of the prediction with and without variance.\n",
    "another_model.make_plots(\"another_sample.png\", var=True) #Variance is included in the plot.\n",
    "another_model.make_plots(\"another_sample_no_var.png\", var=False, title='Experiment with no variance') #Variance is not included.\n",
    "#We can save the optimized model to memory.\n",
    "another_model.save(\"smartphone_model_optimized_2\")\n",
    "end_time = time.time()\n",
    "print(\"Total time for another sample: \", end_time-starting_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
